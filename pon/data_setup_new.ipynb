{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from evaluation.metrics import hit_k, mean_hit_k, average_precision_k, mean_average_precision_k\n",
    "from utils.constants import TRANSCRIPT_COL\n",
    "from utils.helpers import extract_query_from_df\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "max_seq_length=128\n",
    "model = SentenceTransformer(model_name)\n",
    "model.max_seq_length = max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'elastic'\n",
    "pwd = 'password'\n",
    "index_name = 'transcript'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Elasticsearch(\"http://localhost:9200\")\n",
    "client = Elasticsearch(\"https://localhost:9200\",ca_certs=\"ca.crt\",basic_auth=(\"elastic\", \"password\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'es01', 'cluster_name': 'docker-cluster', 'cluster_uuid': '43w2srkGTOOeCr5P57l5Hg', 'version': {'number': '8.12.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '48a287ab9497e852de30327444b0809e55d46466', 'build_date': '2024-02-19T10:04:32.774273190Z', 'build_snapshot': False, 'lucene_version': '9.9.2', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['optim1.csv', 'optim2.csv', 'optim3.csv', 'optim4.csv']\n",
      "['signal1.csv', 'signal10.csv', 'signal2.csv', 'signal3.csv', 'signal4.csv', 'signal5.csv', 'signal6.csv', 'signal7.csv', 'signal8.csv', 'signal9.csv']\n",
      "['stat1.csv', 'stat2.csv', 'stat3.csv', 'stat4.csv', 'stat5.csv', 'stat6.csv', 'stat7.csv', 'stat8.csv']\n"
     ]
    }
   ],
   "source": [
    "for sub in ['optim','signal','stat']:\n",
    "    print(os.listdir(f'../data/all/{sub}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id = 0\n",
    "# index_name = 'transcripts'\n",
    "# for sub in ['optim','signal','stat']:\n",
    "#     for file in os.listdir(f'../data/all/{sub}'):\n",
    "#         filename = f\"../data/all/{sub}/{file}\"\n",
    "#         df = pd.read_csv(filename, index_col=0)\n",
    "#         query_dict = extract_query_from_df(df)\n",
    "#         corpus_embeddings = df[TRANSCRIPT_COL]\n",
    "#         corpus_embeddings = model.encode(\n",
    "#             corpus_embeddings, convert_to_tensor=False, show_progress_bar=True\n",
    "#             )\n",
    "#         df['embeddings'] = list(corpus_embeddings)\n",
    "#         df['clip_id'] = id\n",
    "#         id+=1\n",
    "#         documents = df.to_dict(orient='records')\n",
    "#         # Index documents into Elasticsearch\n",
    "#         for document in documents:\n",
    "#             # Index document into Elasticsearch\n",
    "#             client.index(index=index_name, body=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 3190 Hits:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m Hits:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hit \u001b[38;5;129;01min\u001b[39;00m resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%(timestamp)s\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m%(author)s\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;132;43;01m%(text)s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhit\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_source\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'timestamp'"
     ]
    }
   ],
   "source": [
    "resp = client.search(index=\"transcripts\", query={\"match_all\": {}})\n",
    "print(\"Got %d Hits:\" % resp['hits']['total']['value'])\n",
    "for hit in resp['hits']['hits']:\n",
    "    print(\"%(timestamp)s %(author)s: %(text)s\" % hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = [\n",
    "    (5,'การ convolution'),\n",
    "    (10,'การบ้าน'),\n",
    "    (8,'linear optimization'),\n",
    "    (4,'fourier transform'),\n",
    "    (0,'aliasing'),\n",
    "    (2,'integer programming'),\n",
    "    (7,'signal สอบอะไร'),\n",
    "    (6,'epsilon greedy'),\n",
    "    (1,'power'),\n",
    "    (9,'optimization')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_time = []\n",
    "for id,query in query_list:\n",
    "    query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "    query = {\n",
    "            \"knn\": {\n",
    "                \"field\": \"embeddings\",\n",
    "                \"query_vector\": query_embedding,\n",
    "                \"num_candidates\": 20,\n",
    "                \"filter\": {\n",
    "                    \"term\" : { \"clip_id\" : id }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    start_time = time.time()\n",
    "    resp = client.search(index=\"transcripts\",size=10, query=query)\n",
    "    end_time = time.time()\n",
    "    search_time.append(end_time - start_time)\n",
    "    # for hit in resp['hits']['hits']:\n",
    "    #     print(hit[\"_source\"][\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
